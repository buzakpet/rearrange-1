INTRO TO MODULE 2- SLOWNESS
A problem that we have to deal with a lot when working in IT is things being slow. This could be our computer, our script or even complex systems. Slow is a relative term. Modern computers are much faster and can do many more things that computers of decades ago. Still, we want them to be much faster, and to do more in less time.
With modern day computers, it might seem that our resources aren't limited, but if we try hard enough, we can hit the limit. For example, when your browsing in your browser and open a new tab, it might seem as if it doesn't take any resource at all, but if you keep opening tabs, at some point, your computer will become sluggish. Depending on your computer and the program you're running, it might take about 100 tabs to get there, but eventually you'll run out of memory and eveything will slow down.
There's a bunch of different things we can do when our system is slow, but the most obvious one is closing any application we don't need at the moment.This works because it help free up some of the resources in our computer, like cpu time, ram, or video memory, that way, the program that we want to run faster will have access to more of this resources. When closing apps that we don't need, we might even have to lookat applets, puggings, extensions or other small programs that might seem harmless as the take some resources to run. 
On top of that, closing other elements that take resources like browser tabs or open files in a document editor can also help.
But all this can take you so far because there is a ton of other reasons why our devices might be slow.
We will look into what causes slow scripts, slow computers, or slow systems.
We will give tools to help identify the most common cause of slowness, and apply solutions to increase the overall performance.

WHY IS MY COMPUTER SLOW?
Our computer executes thousand of millions of instrusctions per second, each of those instructions does one small thing, like incrementing the value, comparing two values, or moving a value from one place to another.
Still with thousands of millions of instructions per second, there is still a lot a computer can do in a second. This allows our computer to seemingly execute a number of things at the same time.
Say your browsing the web, and also running an app that plays your favorite music in the back ground, even if your computer has a single core to execute those apps, it would seem like the computer is running those two programs at the same time. What is happening under the hood is that each app is getting a fraction of the cpu time, and then the next app gets a turn. But most of the time, if we run too many application or one of this application needs more cpu time than the fraction its getting, things might become frustratingly slow.
The general strategy for addressing slowness is to identify the bottleneck in our device, our scripts, or our system to run slowly.
The bottleneck could be the cpu time, as we mentioned, but it could also be the time spent reading data from disk, waiting for data to be transmitted over the network, moving data from disk to ram or some other resource thats limiting the overall performance. We could speed things up by getting rid of anything using the same resources on the same computer.
So if the problem is that your program needs more cpu time? you can close other running programs that you don't need right then, If the problem is that you don't have enough space on disk? You can uninstall applications that you don't use or delete or move data that doesn't need to be on that disk. If the problem is that the app needs more network bandwith, you can try stopping other process that also use the network and so on. 
This only helps if there are too many processes trying to use the same resource.
If we have closed everything that we need to and the computer is still slow, we need to look at other possible explanations. what if the hardware we use isn't just good enough to run the applications we are running on it? In cases such as this, we have to upgrade the underlying hardware. But to make a difference in the result of the performance, we need to make sure that we are actually improving the bottleneck, and not just wasting our money on new hardware that will go unused.
So, how can we tell which piece of hardware needs to be changed? We need to monitor the usage of our resources to know which one is being exhusted. This means its being used completely and programs are getting blocked by not having access to more of it. Is it the CPU, the memory? the disk I/O? The network connection? The graphics card? To find out, we use the tools available in our operatin system, monitor the usage of each resource, and workout which one is blocking our programs from running faster.
We have already seen 'top' in action, which lets us see which processes are using the most cpu time. And if we start by memory, whcih ones are using the most memory? It also shows a bunch of other loading information related to the current state of the computer, like how the processes are running and how the cpu time and memory is being used.
We also listed a couple of other programs like 'iotop', and 'iftop', that can help us see which processes are currently using the most disk I/O usage or the most network bandwith.
On MacOS, the OS ships with a tool called Activity Monitor, which lets us see which processes is using the most cpu, memory, energy, disk or network.
On Windows, there's a couple of OS tools called Resource Monitor or Performance Monitor, which lets us analyze what is going on with the different resources in the computer, including cpu, memory, disk and network.
So if you are looking to diagnose what is making your computer slow, the first step is always to open one of this tools, checkout whats going on, and try to understand which resource is the bottleneck and why, and then plan how to solve the issue.
Ofcourse not all performance problems are solve by closing applications or getting better hardware, sometimes we need to figure out what the software is doing wrong and where its spending most of its time to figure out how to make it faster.
We need to really study each problem to get to the root cause of the slowness.


HOW COMPUTER USE RESOURCES
We have considered the different resources used by our computer, and how the could cause slowness. We have talked about how we need to eliminate the bottleneck and get out computer to better use its resources to boost systems performance.
To do this we need to understand how each component interacts with the other, and what the limitations are. In particular, when thinking about making things faster. Its important to know the different speeds of the parts involved.
When an application is accessing some data, the time spent in accessing that data, will depend on where its located. If its a variable that currently being used in a function, the data will be in the cpu's internal memory, and our program will retrieve it really fast. if the data is related to a running program, but maybe not the current executing function, it will likely be in RAM, and our program will get it to fast also. If our data is in a file, our program will need to read it from disk which is much slower than reading it from RAM. Worse than reading from disk, is reading from over the network. In this we have a lower transmission speed, and we also need to establish a connection with the other end point to make the transmission possible. All this adds to the total time needed to get to the data.
So, if you have a process that requires repeatedly reading data over the network, you might figure out if you can read it once, store it on disk, and then read it from disk afterwards. Or similarly, if you're repeatedly reading files from disk, you might see if you can put the same data directly into the process memory and avoid loading it from disk everytime. i.e you want to see if you can create a cache.
Cache: Stores data in a form that's faster to access that its original form.
There's a ton of cache examples in IT, a web proxy is a form of cache. It stores websites, images, or videos that are access often by users behind a proxy, so they don't need to be downloaded from the internet everytime.
DNS services usually implement a local cache for the websites the resolve, so they don't need to query from the internet whenever someone asks for their IP address. But operating systems also takes care of some caching for us. It tries to keep as much info as possible in RAM so that we an access it fast. This include the contents of files or libraries that are accessed often even if the aren't used right now. We say that this contents are cached in memory.
We called out that, if a data is part of a program currently running it will be in RAM. But RAM is limited. If we run enough programs at the same time, you'll fill it up and run out space.
What happens when you run out of RAM? At first the OS will just remove from RAM anything thats cached but not strictly neccessary, if there's still not enough RAM after that, the OS will put parts of the memory(data in memory) that aren't currently in use unto the harddrive in a space called SWAP. 
Reading and writing from disk is much slower than reading and writing from RAM, so when the SWAPed out memory is requested by an appilication, it will take a while to load it back.
The SWAPing implementation varies accross the different OS' but the concepts is always the same. The info that is not needed right now is removed from RAM and put on the disk, while the info that is needed now is put on RAM.
This is a normal operation and most of the time we don't notice it, but, if the available memory is significantly less than what the running applications need, the OS will have to keep SWAPing out the data that is not in use right now, to move the data currently in use to RAM, and as we called out, our computer can switch between apps very quickly which means that the data currently in use can also change very quickly. The computer will start spending a lot of time writing to disk to make some space at RAM, and then reading from disk to put other things in RAM. This can be super slow.
So what do you do when you find that your machine is slow, because its spending a lot of time swapping? There are basically three possible reasons for this. We've talked about 2 of them already:
1. If there are many open applications close the ones that aren't needed yada yada yada. Or
2. If the available memory is too small for the amount of the computer is using, add more RAM to the computer.
3. One of the programs may have a memory leak causing it to take all the available memory.
Memory leak: Memory which is no longer needed is not getting released.
For now, suffice it to say that if a program is using a lot of memory, and this stops after you restart the program. Its probably because of a memory leak

POSSIBLE CAUSES OF SLOWNESS
We have considered a few things that could make our computer slow, but we know that a there are a lot more reasons.
Let give a run down of the most common ones we might come across in our IT role.
A quick reminder first that when trying to diagnose why a computer is slow, we should use the process of elimination that we looked at earlier. We first look for the simplest of explanations that are the easiest check, and after eliminating a possible root cause, we go back to the problem and come up with the next possible cause to check.
So the first step in figuring th causes of slowness is:
1. To look into when the computer is slow. If its slow when starting up, its probably because there are two many applications configured to start on boot. In this case, fixing the program is just going throught the list of programs that start automatically and disabling any of them that are not really needed.
If instead the computer becomes sluggish after days of running just fine and the problem goes away with a reboot, it means that there's a program that is keeping some state while running that's causing the computer to slow down. 
For example this could happen if a program stores some data in memory and the data keeps growing overtime without deleting old values. If a program like this stays running for many ays, the data might grow so much that reading it becomes slow, and the computer runs out of RAM. This is almost certainly a bug in the program and the ideal soltution is to change the code so that it frees up some of the memory in use. If you don't have access to the code, another option is to schedule a regular restart to mitigate both the slow program and your computer running out of RAM.
A similar problem that can trigger after using application for a long time and is'nt solved by reboot is that the files that an app is handling has grown too large, so when the program needs to read those files it getts really slow. Again, this generally points to a bug in the way the program was designed. because it didn't expect the files to grow so large. The best solution is to fix the bug. But what happens when you can't modify the code in the program? You can try to reduce the size of the files involved. If the file is a log file, you can use a program like 'Logrotate' to do this for you.
For other formats, you might need to write your own tool to rotate the contents.
Another data poin that we can use to diagnose what is going is whether this happens for all users of the application or just a subset of them. If only some users are affected, we'll want to know if there's something that is configured differently on those computers, that might be triggering slowness. For example, many operating system include a feature that tracks the files in our computer, so its easy and fast to search for them. This feature can be really usefull when looking for something on the computer, but can get in the way of everyday use if we have tons of files, and not the most powerfull hardware.
We have pointed out before that reading from the network is notably slower than reading from the disk. It's common for computers in an office network to use a file system that is mounted over the network, so that the can share files across the computers. This normally works just fine, but can make some programs really slow if they're doing a lot of read and writes on this network mounted file system.
O fix this, we need to make sure that the directory used by the program to read and write most of its data is a directory local to the computer.
Hardware failures can also cause our computer to become slow. If your hard drive has errors, the computer might still be able to apply error correction to get the data that it needs, but it will affect the overall performance, and when a hard drive start having errors, its only a matter of time before data starts getting lost. So its worth keeping an eye out for faulty hard drives.
To do this, we can use some of the OS utilities that diagnose problems on hard drives or RAM, and check if there is anything that could be causing problems.
Yet, another cause of slowness is malicious software. We can feel the effect of malicious software even when they're not installed. For example, you might have come across a website that includes scripts either in the websites contents or the ads displayed that use our processor to mine for cryptocurrency. Malicious browser extension also fall into this category.
Whenever we have to fix an issue like this we need to look at what the bottleneck is, figureout the root cause behind the resource being used up, and then take appropriate action.

SLOW WEB SERVER
A user has alerted us that one of the web services in our company is being slow and we need to figure out was is going on.
Lets start by navigating to the website and loading the page

site.example.com

We see that the page loads, but it seems to be a litle slow, but its hard to measure this on our own. Lets use a tool called 'ab' which stands for Apache Benchmark to figure out how slow it is. We'll run 'ab -n 500' to get the average timing of 500 request, and then pass our site.example.com/ for the measurement.

ab -n 500 site.example.com/

This tool is super helpful to check whether a website is behaving as expected or not. It will make a bunch or request and summerize the results when its done. Here, we asking it to do 500 request for our website. There are a lot more options we could pass, like how many request we want to the program to do at the same time, or if the test should finish after timeout, even if not all request completed.
We are making 500 request, so that we can get an average of how long things are taking. Once the test finishes, we can look at the data and decide whether it is slow or not.
Okay, so the tool has finally finished running and we can see that the average time per request is 150 ms. While this is not a superhuge number, it is definitely more than what we expect for such a simple website. It does seem like something is going on with the webserver, and we need to investigate what is going on. Lets connect to the web server.

ssh webserver

We'll start by looking at the output of top and see if there's anything suspicious there.

top

We see that there's a bunch of 'ffmpeg' processes running, which are basically using all the available cpu. See those load numbers, 30 is definitely not normal. Remember that the load average shows how times a processor is busy in a given minute. With 1 meaning it was busy for the whole minute. This computer has two processors, so any number above means that it is overloaded, during each minute, there were more processes waiting for processor time than the processor had to give. This ffmpeg program is used for video transcoding, which means converting files from one video format to another.
This is a cpu intensive process and seems like the likely culprit for our processor being over loaded.
So what can we do? Well one thing we can do is to change the process priorities so that the web server takes precedence. The process priorities in Linux are so that the lower the number, the higher the priority. Typical numbers go from 0 to 19, by default, processes start with a priority of 0, but we can change that using the 'nice' and 'renice' commands. We'll use 'nice' for starting a process with a different priority, and 'renice' for the changing the priority of a process that it already running.
So lets exit 'top' with 'q' and change the priorities.

q

We want to run renice for all the ffmpeg processes running right now. We can do this one by one, but this will manual, error prone, and super boring. Instead, we will use a quick line of shell script to do this for us. For that we will use the 'pidof' command that recieves a process name, return all the process IDs that have that name. Well iterate over the output of the 'pidof' command with a for loop and then call 'renice' for each of the process IDs. 'renice' takes the new priority as the first argument, and the process ID to change as the second one. In our case, we want the lowest possible priority, which is 19.

for pid in $(pidof ffmpeg); do renice 19 $pid; done

We see that the process IDs for those processes are updated. Lets run benchmarking software again and see if it made any difference.

ap -n 500 site.example.com

This time, the average time is 153ms, it doesn't seem like our 'renice' helped. Apperently, the OS is still giving this ffmpeg process much of the processor time and our website is still slow. This transcoding processes are cpu intensive, and running them in parallel is overloading the computer. 
What we can do is modify whatever is triggering them to run them one after the other, instead of all at the same time. To do that we need to know how this processes got started. To do that we look at the output of th ps command to get some more info about the processes. We'll call 'ps ax' which shows us all running processes on the computer, and we'll connect the output of the command to 'less' to be able to scroll through it.

ps ax | less

Now we'll look for the ffmpeg process using '/', which is the search key when using 'less'

/ffmpeg

We see that there are bunch of ffmpeg processes that are converting videos from the .webm format to the mp4 format. We don't know where this vidoes are on the hard drive. We can try using the 'locate' command to see if we can find them.

q
locate static/001.webm

This is what we get '/srv/deploy_videos/static/001.webm'. This means that what we wanted to locate is situated in the the directory '/srv/deploy_videos'.
Let change into that directory

cd /srv/deploy_videos/
ls -l

There a bunch of files here. We can check them one by one to see if anyone contains a call to ffmpeg, but that sounds like a lot of manual work. Instead lets use grep and see if any of this files contains a call to ffpeg

grep ffmpeg *

So, we see that there are a couple of mentions in the deploy.sh file, lets take a look at that one. Since we are connecting to the server remotely, we can't open the file using a graphical editor. We need to use a command-line editor instead, we'll use vim in this case.

vim deploy.sh

We can see that this script is sarting the ffmpeg processes in parallel using a tool called Daemonize that runs each program seperately as though it were a daemon. This might be ok, if we wanted only to convert a couple of videos, but one seperate process for each of the videos in the static directory is overloading our server. So we want to change this to run only one video conversion process at a time. We'll do this by simply deleting the daemonize part and keeping the part calls ffmpeg, save and exit.
So, we've modified the file, but this won't change the processes of the ones that are already running. We want to stop this processes, but not cancel them completely because doing so would mean that the vieos currently being processed will be incomplete.
So, we'll use the 'killall' command with the '-STOP' flag, which sends STOP signal, but doesn't kill all the processes completely

killall -STOP ffmpeg

We now want to run this processes one at a time? We could send the 'CONT' signal to one of them and wait till its done and then send it to the next one, so we'll automate it. It can be tricky so pay good attention...
We can iterate over the processes with a for loop and use the 'pidof'. Inside the for loop we want to send the CONT signal and wait till the process is done, unfortunately, there is no command to wait till the process finishes, but we can create a while loop that sends the CONT signal to the process. This will succeed as long as the process exists, and fails once the process goes away. Inside the while loop, we'll add a call to sleep 1, to wait one second until the next check

for pid in $(pidof ffmpeg): do 
  while kill -CONT $pid; do 
    sleep 1; 
  done; 
done

So, lets try our bench mark once more, the mean time is now 33 ms, thats much lower than before.

WRITING EFFICIENT CODE
No matter the size and complexity of our code, we usually want it to perform well.
In this video and the next few ones, we will discuss some ideas for how to make our code more efficient and how to figure out what needs fixing if its slow.
***Note We should always start by writing clear code that does what it should, and only try to make it faster if we realize it's not fast enough.
If it takes you 10 minutes to write a script that will run in 5 seconds, and 20 minutes to write a script that will do the same, but take three seconds, does it make a difference? It all depends on how often you run the script.
If you run it once a day, the extra 2 seconds does not justify the extra 10 minutes of work, but if you're going to run the same scripts for the 500 computers on your network, that small difference means it will take 15 less minutes to run the whole scripts, so overall you've gained 5 minutes.
But it's hard to know in advance how fast your script will be, and how long it will take to make it faster. But as a rule, we aim to write code that is readeable, easy to maintain, and easy to understand because that lets write code with less bugs. If there's something thats supper slow, then yes, it makes sense to fix it, particularly if the script will be executes frequently enough, that making it faster will save you more time than the time you spend optimizing it.
But remember, trying to optimize every second out of a script is probably not worth your time.
Lets now dive into how we can make our code more efficient.
The first step is to keep in mind that we can't actually make our computer do faster. If we want our code to finish faster, we need to make our computer do less work. and to do this, we have to avoid doing work that isn't really needed. How? There's a bunch of things to do. The most common ones include:
a.Storing date already calculate to avoid calculating them again,
b.Using the right data structure for the problem,
c.And reorganizing the code so that the computer can stay busy while waiting for data from slow sources, like disk or over the network.
To figure out what sources of slowness we need to address, we have to figure out were code is spending most of its time? There's a bunch of tools that can help us with that called profilers.
Profiler: A tool that measures the rsources that our code is using, giving us a better understanding of what's going on.
In particular, the help us see how memory is allocated and how the time is spent.
Because of how profilers work, they are specific to each programming language, so, we'll use 'gprof' to analyze a C program, but use the cProfile module to analyze a python program.
Using tools like this, we can see which functions were called by our program, how many times each function was called, and how much time our program spent on each of them. This way we can find for example, that our program is calling a program more times than originally intended, or, that a function we thought will be fast is actually slow. To fix our code, we'll probably restructure it to avoid repeating expensive actions. What do we mean by expensive? In this context, expensive actions: Those that can take a long time to complete.
Expensive operations include parsing a file, reading data over the network, or iterating through a whole list. And how do we modify our code to avoid expensive operations? Next video

USING THE RIGHT DATA STRUCTURE
Having a good idea of the data structure available to us can help us avoid expensive, unnecessary operations, and create effecient scripts. In pariticular, we'll want to understand the performance of those structures under different conditions.
You have learnt a bunch of data structures in python, like: list, dictionaries, tuples, and sets. Each of them have their uses, advantages and disadvantages.
Lets recap on list and dictionaries.
List: Sequences of elements. We can add, remove, or modify the elements in them, and we can iterate through the whole list to operate on each of the elements.
Different programming languages call them differently. The structure is called ArrayList in Java, Vector in C++, Array in Ruby and Slice in Go. All this names refere to the same data structure that's fast to add or remove data elements at the end. But adding and removing elements in the middle can be slow, because all the elements that follow, needs to be repositioned. It's fast to access the elements in a specific position in a list. But finding an element in an unknown position requires going through the whole list. This can be super slow if the list is long.
Dictionaries: Stores key-value pairs. We add data by associating a value to a key, and then we retrieve a value by looking up a specific key.
They called HashMap in Java, Unordered Map in C++, Hash in Ruby and Map in Go.
The 'map' in those names comes from mapping, which is how we create a key and value. The hash part comes from the part that to make the structure efficient, a hashing function is used internally to decide how the elements will be stored.
The main characteristics of this structure is that it is super fast for looking up keys, once we have our data stored in a dictionary, we can find a value associated to a key in just one operation. If it were stored in a list, we need to iterate through the list.
So as a rule of thumb, if you need to access elements by position, or, if you will always iterate through al the elements, use a list to store them.
This could be a list of all computers in a network, of all employees in a company, or of all products currently on sale for example.
On the flipside, if we need to look up the elements using a key, we'll use a dictionary. This could be the data associated to a user, which we look up using their user name, the IP associated to a computer, using the host name, or the data associated to a product, using the internal product code.
Whenever we need to do a bunch of this look up operations, creating a dictionary, and using it to get the data will take a lot less time, than iterating over a list to find what we are looking for.
But it doesn't make sense to create a dictionary, and fill it with data if we are going to look up just one value in it. In that case we are wasting time creating the structure, when we could just iterate over the list and get the element we are looking for.
Does this mean that dictionaries are schmiddlebach?
Another thing that we want to think twice about is creating copies of the structures that we have in memory. If this structures are big, it might be expensive to create those copies. So we need to check that those copies are really needed.

EXPENSIVE LOOPS
We need to use loops with caution. In particular, we need to think of what actions we are going to do inside the loop, and when possible, avoid doing expensive actions.
If you do an expensive operation inside a loop, you multiply the time it takes to do the expensive operation by the amount of times you repeat the loop.
Say for example that you are writing a script to send an important email to all the employees of your company, asking them to verify that their emergency contact info is still valid. To send this out, we have a loop that sends out one email per employee, and in the body of the email, you include the emergency contact email data. The interesting part is how you access the data inside the loop. If your data is stored in a file, your scripts will have to parse the file to fetch it. If the scripts reads the whole file for every user, you'll be wasting a lot of time parsing the file over and over unnecessarily, instead, you an parse the file outside the loop, put the info into a dictionary and then use the dictionary to retrieve the data inside the loop.
So whenever you have a loop in your code, make sure to check what your loop is doing and see if there are operations you can take outside your loop, to do them just once.
Instead of making one network call for each element, make one call before the loop, instead of reading from disk for each element, read the whole thing before the loop. And even if the operations done inside the loop are not especially expensive, if were gooing through a list of 1000 elements and we only need 5 out of them, were waisting time on elements we don't need.
So, make sure that the list of elements that you're iterating through is only as long as you really need it to be.
Lets say we're running an internal website as part of the information that the website shows, it displays a list of the last 5 users that logged in. In the code the program keeps a list of all the users that have logged in since it last started, and when the program needs to display the last 5 users, it goes through the whole list and brings out which are the 5 most recent. This wastes a lot of time, and if the service has been running for some time, it can take really long to go through the whole list. Instead you can modify the service to store the users access involved in log files that can be read if necessary and only keep the last 5 logins in memory. So when ever a new user logs in, the oldest entry in the list is discarded and the new one gets added, that way, the script doesn't have to go through the whole list any time it want to display the 5 most recent logins.
Another thing to remember about loops is to break our of the loop once you've found what you were looking for. In python, we do this using the keyword break.
Breaking out of loops mean that as soon the data we are looking for is found ourscripts can continue. Ofcourse, if the data is at the end of the loop, then we need to go through the loop anyway. But when the data is at the beginning of the list, it makes sense to have our code break early, to make the scripts faster. 
Say you are writing a scripts tha checks if a given username is within a list of authorized entities, and if it is, it grants them access to a particular resource. You can use a for loop to iterate through a list of entities, and when the username is found, you can break out of the loop and continue the rest of the scripts.
One last thing to keep in mind is that the right solution for one problem might not be right for a different problem.
Say your sevice has a total of 20 users, in that case, its okay to go over the list whenever you want to dheck something, its short enought that you don't need to do any special optimization, but if your service has over a thousand users, you might want to avoid going through that list unless absolutely necessary. And if the service has hundreds of thousand of users, going through that list isn't even a possiblity.

KEEPING LOCAL RESULTS
In our last video, we talked about how to avoid epensive operations inside our loops. We talked about parsing files outside loops to avoid expensive operations.
But what if parsing the file is taking a lot of time even outside the loop? Remember that to make scripts get to their goal faster we need to avoid having our computer do unnecessary work. So how can we avoid expensive operations like parsing a file, downloading data over the network, or going through a long list? If the scripts get executed fairly regularly, its common to create a local cache. So if we are parsing a large file and only keeping a few pieces of info from it, we can create a cache to store only that info, or if we are getting some info over the network, we can keep a local copy of the file to avoid downloading it over and over again.
Creating caches can be very useful to make our programs faster and to save time. But they are some times tricky to get it right. We need to think of how often we're going to update the cache and what happens if the data in the cache is out-of-date?
If we are looking for some long-term stats, we can generate the cache once per day and it won't be a problem. This might be the case data like: how much memory on computer was used across the fleet over the last month, how many employees each department in a company has, or how many units were sold of each product over the last quarter. But if we are trying to look at data where the value as of right now, is super important, we either can't use a cache, or it has to be very short-lived. This could be the case of monitoring the health of computers to alert when something crosses a threshold, checking the stock level to see if there's enough of a product to sell, or seeing if a username already exists in a network when trying to create a new one. 
Somtimes we can add a check to validate if we need to recalculate the cache or not. For example if our cache is based on a file, we can store the modification date of that file when we calculated the cache, and then, only recalculate the cache if the modification date of the file is newer than the one we had stored.
If we don't have a way of checking if our chache is out of date or not, we'll need to add in logic to our program that tries to make a sensible decision. For that, we'll take into account how often we expect the data to change, how critical it is that the latest data is used, and how frequently that the data we are running we'll be executed. After taking all this factors into account, we might decide that the cache needs to be recreated once per day, once per hour, or even once per minute. Once per minute might make sense if have a script that can get executed several times per minute and needs to do an expensinve operation that can be cached. That way, only the first execution in a minute will spend time on this operation, the rest will very fast, but the cache is never over a minute out of date.
And keep in mind that caches don't alway have to be ellaborate strucutures, storing lots of information with a complex time out logic. Sometimes, they can be as simple as having a variable tha stored a temporary result instead of calculating this result everytime we need it.
For example say we are generating a report that prints how many users they are in each of the different groups, in the network. Now some of this groups may contain other groups in the network, and some groups may even be part of several groups, for example, the 'Java Release eEgineers Group', will be part of the 'Release Engineers Group' and the 'Java Developers Group'. How can we avoid unique users more than once if the show up in multiple groups? We can have a dictionary with the group as the key and the amount of users as the value, that way, we only need to count the members of a group once, and after that, just use the value in the dictionary.
To sum up, you'll want to come up with an effective strategy to deal with expensive operations.
Check if this operations are needed at all, and if they are, see if you can store the intermediate results to avoid repeating the expensive operation more than needed.

SLOW SCRIPT WITH EXPENSIVE LOOP
Remember that meeting_reminder.sh script that was having trouble with the date, it now sends personalized emails, with the name of the person getting the email and the greeting. Thats cool, but it seems to have made the application slow. The developers are asking for our help to see how they can make the program faster, so lets get to work.
First we'll need to reproduce the prblem and figure out what slow means in this case. One user told us that the problem is visible when the list or recipients is long.
To avoid spaming our colleagues while testing this issue, we'll send our reminders to a bunch of test users we've created in our mail server.
You might remember that the app has two parts, the shell scripts that pops up a window were we can enter the data of the reminder, and the python scripts and the python scripts that prepares the email and sends it. The part thats slow is the sending of the emails, so we won't interact with the pop up at all. We'll just pass the parameter we need into the python script. We'll measure the scripts speed using the time command

/send_reminders.py "2020-01-13|Example|test1"

When we call time, it runs the command that we pass to it and print how long it took to execute it. There is three different values, real, user, and sys.
Real: The amount of actual time it took to execute the command. This value is sometime called Wall-clock time., because it how much time a clock hanging on the wall would measure no matter what the computer is doing.
User: The time spent doing operations in the user space.
Sys: The time spent doing system-level operations.
The values of user and sys won't necessary add-up to the value of real, because the computer might be busy with other processes.
So what we see is that, it took our scripts 0m0.129s to send the email. Thats not a lot, but we only sent it to one user. Lets try the scripts with our 9 test users

./send_reminders.py "2020-01-13|Example|test1,test2,test3,test4,test5,test6,test7,test8,test9"

We see that it took 0m0.296s. Thats still not enough, but it does look like it takes longer with a longer list of emails.
Okay, its time to make the code better, how can we find out whats wrong with the code. We can alway look at the code and see if we find any expensive operations so we can improve, but in this case, we want to use a profiler to get some data about what is going on. So lets do that.
There are a bunch of different profilers available for python that work for different use cases, here we use the one called 'pprofile3'. We'll use the '-f' flag to tell it to use the callgrind file format and the '-o' flag to tell it to store the output in the profile.out file.

pprofile3 -f callgrind -o profile.out ./send_reminders.py "2020-01-13|Example|test1,test2,test3,test4,test5,test6,test7,test8,test9"

This generated a file that we can open with any tool that supports the callgrind format. We're going to use the kcachegrind to look at the contents, which is a traphical interface for looking into this files.

kcachegrind profile.out

There's actually a lot going on with this program. Lets look at the info we need now. In the lower right half, we see call graph which tells us that the main function is calling the send_message function one time. This send_message function is calling message_template function, get_name function, and the send_message function 9 times each. The graph also tells us how many micro-seconds are spent on each call.
We can see that most of the time is being spent in the get_name function. Thats probably the one we should optimize. Lets see what this function is doing using atom

atom send_reminders.py

def get_name(contacts, email):
  name = ""
  with open(contacts) as csvfile:
    reader = csv.reader(csvfile)
    for row in reader:
      if row[0] == email:
        name = row[1]
  return name

So we see that the get names function opens a csv file, then goes through the whole file checking if the first field matches the email name, and if thats the case, it sets the value of the name variable.
There are a couple of things that are wrong with this function, first, once it finds the elements in the list it should immediately break out of the loop. Right now, its iterating over the whole file even if the email was found in the first line. But even if we fix that it will open the file and read through it for each email address. This can get really slow if the file has a lot of lines. 
So how can we make this better? and store the values we care about in a dictionary, and then use that dictionary for the lookups. Lets do that.
We'll change the get_names function into a read_names function that will process the csv file and store the values we want in the names dictionary. For each line, we'll store the email as the key, the names as the values, and instead of returning 1 name, we'll return the whole dictionary.

def read_names(contacts):
  names = {}
  with open(contacts) as csvfile:
    reader = csv.reader(csvfile)
    for row in reader:
      names[row[0]] = row[1]
  return names
  
We now have a read_names function that stores the names we want to read in a dictionary. We now want need to change the this is called in the send_message function. We see that the get_name function inside the send_message function is being called once per emmail.

def send_message(date, title, emails, contacts):
        smtp = smtplib.SMTP('localhost')
        for email in emails.split(','):
                name = get_name(contacts, email)
                message = message_template(date, title, name)
                message['From'] = 'noreply@example.com'
                message['To'] = email
                smtp.send_message(message)
        smtp.quit()
        pass

To aplly our changes in the send_message function, we need to call the read_names function before the foo loop, so that we do it only ones, and instead of calling get_name, we'll just get the value from the dictionary.

def send_message(date, title, emails, contacts):
        smtp = smtplib.SMTP('localhost')
        names = read_names()
        for email in emails.split(','):
                name = names[email]	
                message = message_template(date, title, name)
                message['From'] = 'noreply@example.com'
                message['To'] = email
                smtp.send_message(message)
        smtp.quit()
        pass

Now that we've changed the function, lets save and try our scripts again to see if we managed to make it any faster.

pprofile3 -f callgrind -o profile.out ./send_reminders.py "2020-01-13|Example|test1,test2,test3,test4,test5,test6,test7,test8,test9"
kcachefrind profile.out

THe graph looks different now as we've changed how the code behaves. See how the read names function is taking a much small portion of time. We see that the message template is the one taking most of the time now. So, if we want to keep making our scripts move faster, thats the function we look at next.
Summary:
In this video, we saw how we can use the time command to check how long it takes to execute a program. We then saw how to combine a profiler and a profile visualizer to figure out were our code is spending most of its time. And finally we change our code to avoid doing an expensive loop over and over, by storing info in a dictionary and accesing the dictionary instead.

PARALLELIZING OPERATION
We have mentioned that reading info from disk or getting info over a network is a slow operation, In typical scripts, while this operation is going on, nothing else happens. The script is blocked waiting for input or output, while the cpu sits idle.
One way we could make this better is by doing operations in parallel. That way, when the computer is waiting for the slow I/O, other work takes place.
The tricky part is diving the task so that we get the same result in the end.
There's actually a whole field in computer science called concurrency, dedicated to how we write programs that do operations in parallel. But we will give a brief overview of what we can do.
First, we need to understand what the operating system already does for us. Our OS handles the many processes that run on our computer. If our computer has more than 1 core, the OS can decide which processes gets executed on which core, and no matter the split between cores, all the this process will be executing in parallel. Each of them has its own memory allocation and does its own I/O calls. The OS will decide what fraction of CPU time each process gets and switch between them as needed.
So, a very easy way to run processes in parallel, is just to split them across processes. Calling your scripts many times each with a different input set and just let the OS handle the concurrency.
Lets say ou want to collect statistics on the current load and memory usage, for all the computer in your network. You can do this by writing a script that connects to each computer in a list and gets the stats, each connection takes a while to complete and so the total run time of the scritps will be the sum of the time taken by all those connections. Instead, you can split the list of computers into smaller groups, and use the OS to call the scripts many times, once for each group, that way, the connections to the different computers can be started in parallel, which minimizes the time that the CPU isn't doing anything.
This is very easy to do, and for manu scripts, this will be the right choice.
Another easy thing to do is to have a good balance of different workloads that you run on a computer. If you have a process that is using a lot of cpu and another process that is using a lot network IO, and another process is using a lot of disk IO, this can all run in parallel without interfering with each other.
When using OS to split workload into processes, these processes don't share any memory. And sometimes, we might need to have some shared data, in that case we use threads.
Threads: Let us run parallel tasks inside a process. This allows threads to share some of the memory with other threads in the same process. Since this isn't handled by the OS, we'll need to modify our code to create and handle the threads. For this, we'll need to know how the programming language we are using implements threads. In Python, we can use the Threading, or Asyncio module to this. This modules lets us specify which part of the code we want to run in seperate threads, or in a seperate asynchronuous events, and how we want the results of each to be combined in the end.
***Note, one thing to watch out for is that, depending on the actual threading implementation for the language you're using, it might happen that all threads get executed in the same cpu processor. In that case, if you want to use more processors, you'll need to seperate the code into fully seperate processes.
If your scripts is mostly waiting on I/O, also known as IO bound, it might not matter if its executed on 1 processor or 8. But you might be doing this in parallel because you are using all the available cpu time. In other words, your scripts is cpu bound. In this case, you'll definitely want to split your execution across processors.
Now there's a point where adding more parallel processes means things become even a bit slower not faster.
If we are trying to read a bunch of files from disk and do too many operations in parallel, the disk might end up spending more time going from one position to another that actually retrieving the data.
Or, if we are doin a ton of operations that use a lot of cpu, the OS could spend more time switching between them than actually making progress in the calculations we are trying to do.
So when we are doing operations in parallel, we need to find the right balance of simultaneous actions that let our computer sta busy without starving our system for resources. 
I recently felt the benefits of applying concurrency. I was working on migrating data that was stored in one format and I needed to store it in a different format. There was a lot of gigabytes of data that needed migrating so, I wasn't going to do it manually. My first version of the script was taking an average of 1 hr per 1gb of data migrated. This was much slower than I expected. So I decided to spend my time twiking the code to make my migration go faster. I reorganized the logic to have a seperate thread per file, which decreased the total time to work through the files since it wasn't now a linear process. And then, to make the work go even faster, I split the work unto different machines each running a bunch of threads. After all this rearranging to use this resources I had, I brought it down to 3 mins per gb.

SLOWLY GROWING IN COMPLEXITY
As we called out earlier, a solution good for one problem might not be good for another. And as a system becomes more complex and grows in usage, a solution that worked before may no longer be well suited.
Lets say you're writing a Secret Santa scripts, were each person gives a secret a gift to one other randomly assigned person. The scripts randomly selects pers of people and then sends an email to the gift giver, telling them who they're buying a present for.
If you're doing this for the people working on your floor, You might just store the list of people and emails in a CSV file. The file will be small enought that the time spent parsing it won't be significant. Now, if this scripts grows into a larger project that involves every one in your company, and your company keeps hiring more and more people, at some point parsing the file will start taking a lot of time. This is when we might want to start using a different technology. For example, you might decide to store your data in an SQlite file.
SQLite is a light weight database system that lets you query the info stored in a file without needing to run a database server.
Using SQLite works just fine for assigning secret santas at the company.
But imagine that you've added features to the service that it now includes a way to add wish list, a machine learning algorithm that suggest possible gifts, and a tracker that keeps a history of each person given, and since people at your company love the program so much, you've made it an external service available to anybody. Keeping all the data in one file will be too slow. So you'll need to move to different solution, so you'll have to use a fully fledged database server, probably even running on a different machine than the one running the secret santa service. And there's even one more step after that, if the service becomes really really popular, you might notice that your database isn't fast enough to serve all the queries being requested. In that case, you can add a caching service, like memcached that keeps the most commonly used result in RAM to avoid quering the database unnecessarily.
So we've gone from having it in a CSV file, to having it in a SQLite file, then moving it to a database server, and finally using a dynamic cacher infront of the database server to make run even faster.
A similar progression can happen on the user-facing side of the project. Initially we said that the service simply send emails to people on the list. Thats fine if its a small group and there is one person in charge of the scripts. But as the project grows more complex, you want to have a website for this service to let people do things like check who their assigned person is, and create wish-list. Initially this could just be running on a web server on the same machine as the data. If the website gets used a lot, we might need to add a caching service like vanish. Vanish will speed up the load of dynamic created pages. And eventually, this still might not be enough. So you'll need to distribute your service across many different computers, and use a load balancer to distribute the request. You can do this in-house with seperate computers hosted at your company, but this means that as the application keeps growing, you'll need to add more and more servers. It might be easier to use VMs running in the cloud that can be added or remove as the load sustained by the service changes.
This examples show how important it is to find the right solution for each problem.
It makes no sense to deploy a multi-server web service with a distributed database for storage when you're going to have only a few dozen users. You just need to pay attention to how the service is growing to know when you need to take the next step to make it work best for the current usecase.
Next, we'll talk about some of the issues we might come across when dealing with complex systems.

DEALING WITH COMPLES SLOW SYSTEMS
We talked last about how systems that grow in usage also grows in complexity. In large complex systems, we have lots of computers involved, each one doing a part of the work and interacting with the others over the network.
For example, think of an e-commerce site for your company. The webserver is the part of your system that directly interacts with external users. Another component is the database server, which is accessed by the code that handles any request generated from the website. And depending on how the whole system is built, you might have a bunch of other services involved doing different parts of the work. There could be a billing system that generates invoices once orders are placed, a fullfilment system used by the employees preparing the orders for customers, a Reporting system that once a day creates a report of all the sales placed, etc. On top of this, we should also have backup, monitoring, testing infrastructure.
A system like this maybe tricky to debug and understand. What do you do if your complex system is slow?
As usuall, what you want to do is find the bottleneck that is causing your system to underperform. Is it the generation of dynamic pages on the web server? is it the queries to the database? Is it doing the calculations for the fulfilment process? Figuring this out can be tricky. So one key peice is to have a good monitoring infrastructure, that lets you know where the system is spending the most time.
Say you noticed that getting the web pages is pretty slow, But when you check the webserver you see that it is not overloaded, instead most of the time is spent waiting on network calls. And when you look at your database server, you find that its spending a lot of time on disk I/O(reading and writing). This shows that there's a problem with how the information is being accessed in the database. One thing to look at is the indexing present in the database.
When a database server needs to find data, it can do it much faster if there's and index in the field you are quering for. On the flip-side, if the database has two-many indexes, adding or modifying entries can be really slow, because all of the indexes need updating.
So, we need to look for a good balance to having indexes to the fields that are actually going to be used. If the problem is not solved by indexing, and there're too many queries for the server to reply to all of them on time, you might have to look into caching the queries, or distributing the data to seperate database servers.
Now, what if when you try to figure out why the service is slow, you see that the cpu of the webserving machine is saturated. The first step is to check if the code of the service can be improved, using the techniques that we explained earlier. If its a dynamic website, we might need to try adding caching on top of it. But if the code is fine and the cache doesn't help because the problem is just that there is too many request coming in for one machine handling all of them, you'll need to distribute the load across more computers.
To make this possible, you might need to reorganize the code so that it is capable of running in a distributed system instead of on a single computer.
This might take some work, but once you've done it, you can easily scale your application to as many request as needed by adding many computers to the system.
And finally, make sure that you absolutely need to do what you're doing. Lots of time as projects evolve, we are left with a scary monster of layers of layers of complex code. If we THINK ABOUT WHAT OUR SYSTEM IS DOING FOR FEW MINUTES we might end up discovering that there was a whole piece that wasn't needed at all, and it was making our server do unnecessary work all along.
Remeber that when we need to deal with complex systems, one of our best tools is to ask colleagues for help.

USING THREADS TO MAKE THINGS GO FASTER
Our company has an e-commerce website that includes a bunch of images of the products for sale. There is rebranding coming up and this means that all of these images will have to be replaced with new ones. This includes both the full size images and the thumbnails. We have a script that creates the thumbnails based on the full-size images, but there is a lot of files to process and our scripts is taking a long time to finish. It looks like its time to take it up a notch and use something else to do the resizing.
We'll start by trying out the current scripts as is, using a set of 1000 test images. We'll execute our program using the time command to see how long it takes.

cd thumbnail_generator/
cat thumbnail_generator.py

#!/usr/bin/env python3

import argparse
import logging
import os
import sys

import PIL
import PIL.Image

from tqdm import tqdm

def process_options():

	kwargs = {
		'format' : '[%(levelname)s] %(message)s'
	}

	parser = argparse.ArgumentParser(
		description = 'Thumbnail generator',
		fromfile_prefix_chars='@'

	)
	parser.add_argument('--debug', action='store_true')
	parser.add_argument('-v', '--verbose', action='store_true')
	parser.add_argument('-q', '--quiet', action='store_true')

	options = parser.parse_args()

	if options.debug:
		kwargs['level'] = logging.DEBUG
	elif options.verbose:
		kwargs['level'] = logging.INFO
	elif options.quiet:
		kwargs['level'] = logging.ERROR
	else:
		kwargs['level'] = logging.WARN

	logging.basicConfig(*kwargs)

	return options

def process_file(root, basename):
	"""Generates a thumbnail of the given image."""
	# Open the image that we need to process
	filename = f'{root}/{basename}'
	image = PIL.Image.open(filename)

	# Generate the thumbnail
	size = (128, 128)
	image.thumbnail(size)

	# Store the thumbnail in the thumbnails folder
	new_name = f'thumbnails/{basename}'
	image.save(new_name, "JPEG")
	return new_name


def progress_bar(files):
	return tqdm(files, desc='Processing', total=len(files), dynamic_ncols=True)


def main():

	process_options()

	# Create the thumbnails directory
	if not os.path.exist('thumbnails'):
		os.mkdir('thumbnails')


	for root, _, files in os.walk('images'):
		for basename in progress_bar(files):
			if not basename.endswith('.jpg'):
				continue
			process_file(root, basename)
	return 0


if __name__ == '__main__':
	sys.exit(main())


time thumbnail_generator.py

It took about 2 seconds(0m1.962s) for 1000 images. It doesn't seem too slow, but there is hundreds of thousands of images to process, and we want to make sure that the process is as fast as possible.
Lets make this go faster by having to process the images in parallel. We will start by importing the future submodule which is part of the concurrent module. This gives us a very simple way of applying Python threads. To be able to run things in parallel, we'll need to create an executor.
Executor: The process that's in charge of distributing the work among the different workers.
Futures module: Provides a couple of different executors; one for using threads and another for using processes. Well go with the ThreadsPoolExecutor for now. Now, the function that does most of the work in this loop is process_file. Instead of calling it directly in the loop, we'll submit it as a new task to the executor, witht the name of the function and its parameters, i.e executor.submit(process_file, root, basename).
Our for loop now creates a bunch of tasks that are all scheduled in tht executor. The executor will run them in parallel using threads, The interesting thing about threads is that th loop will finish as soon as all tasks are scheduled. But it will still take a while until the task are complete.
So will add a message saying we are waiting for all the threads to finish, and then call the shutdown function on the executor. This function waits until all the work in the pool are done and only the shutsdown the executor.


atom thumbnail_generator.py 

#!/usr/bin/env python3
from concurrent import futures
import argparse
import logging
import os
import sys

import PIL
import PIL.Image

from tqdm import tqdm

def process_options():

	kwargs = {
		'format' : '[%(levelname)s] %(message)s'
	}

	parser = argparse.ArgumentParser(
		description = 'Thumbnail generator',
		fromfile_prefix_chars='@'

	)
	parser.add_argument('--debug', action='store_true')
	parser.add_argument('-v', '--verbose', action='store_true')
	parser.add_argument('-q', '--quiet', action='store_true')

	options = parser.parse_args()

	if options.debug:
		kwargs['level'] = logging.DEBUG
	elif options.verbose:
		kwargs['level'] = logging.INFO
	elif options.quiet:
		kwargs['level'] = logging.ERROR
	else:
		kwargs['level'] = logging.WARN

	logging.basicConfig(*kwargs)

	return options

def process_file(root, basename):
	"""Generates a thumbnail of the given image."""
	# Open the image that we need to process
	filename = f'{root}/{basename}'
	image = PIL.Image.open(filename)

	# Generate the thumbnail
	size = (128, 128)
	image.thumbnail(size)

	# Store the thumbnail in the thumbnails folder
	new_name = f'thumbnails/{basename}'
	image.save(new_name, "JPEG")
	return new_name


def progress_bar(files):
	return tqdm(files, desc='Processing', total=len(files), dynamic_ncols=True)


def main():

	process_options()

	# Create the thumbnails directory
	if not os.path.exist('thumbnails'):
		os.mkdir('thumbnails')

	#Create executor for running threads parallel
	executor = futures.ThreadPoolExecutor()
	for root, _, files in os.walk('images'):
		for basename in progress_bar(files):
			if not basename.endswith('.jpg'):
				continue
			executor.submit(process_file, root, basename)
	print("Waiting for all threads to finish.")
	executor.shutdown()		
	return 0


if __name__ == '__main__':
	sys.exit(main())

Great! Lets save our scripts and test it out.

time ./thumbnail_generator.py

Our scripts takes 0m1.252s to run, which is an improvement from last time. See how the user time is greater than the real time?
By using multiple threads, are scripts is making use of the different processors available on the computer, and this value shows the time used on all processors combined.
What do you think will happen if we try to use processes instead of threads?
Lets try this out by changing the executor that we are using from ThreadPoolExecutor to ProcessPoolExecutor.

#!/usr/bin/env python3
from concurrent import futures
import argparse
import logging
import os
import sys

import PIL
import PIL.Image

from tqdm import tqdm

def process_options():

	kwargs = {
		'format' : '[%(levelname)s] %(message)s'
	}

	parser = argparse.ArgumentParser(
		description = 'Thumbnail generator',
		fromfile_prefix_chars='@'

	)
	parser.add_argument('--debug', action='store_true')
	parser.add_argument('-v', '--verbose', action='store_true')
	parser.add_argument('-q', '--quiet', action='store_true')

	options = parser.parse_args()

	if options.debug:
		kwargs['level'] = logging.DEBUG
	elif options.verbose:
		kwargs['level'] = logging.INFO
	elif options.quiet:
		kwargs['level'] = logging.ERROR
	else:
		kwargs['level'] = logging.WARN

	logging.basicConfig(*kwargs)

	return options

def process_file(root, basename):
	"""Generates a thumbnail of the given image."""
	# Open the image that we need to process
	filename = f'{root}/{basename}'
	image = PIL.Image.open(filename)

	# Generate the thumbnail
	size = (128, 128)
	image.thumbnail(size)

	# Store the thumbnail in the thumbnails folder
	new_name = f'thumbnails/{basename}'
	image.save(new_name, "JPEG")
	return new_name


def progress_bar(files):
	return tqdm(files, desc='Processing', total=len(files), dynamic_ncols=True)


def main():

	process_options()

	# Create the thumbnails directory
	if not os.path.exist('thumbnails'):
		os.mkdir('thumbnails')

	#Create executor for running threads parallel
	executor = futures.ProcessPoolExecutor()
	for root, _, files in os.walk('images'):
		for basename in progress_bar(files):
			if not basename.endswith('.jpg'):
				continue
			executor.submit(process_file, root, basename)
	print("Waiting for all threads to finish.")
	executor.shutdown()
	return 0


if __name__ == '__main__':
	sys.exit(main())


By changing the executor to the process pool executor, we tell the futures module that we want to use processes instead of threads for the parallel operation.
Lets save and try this one out now.

time ./thumbnail_generator.py

Wow! This now takes less than a second to finish, and the user time has gone up even more! This is because by using processes, we are making even more use of the cpu.
The difference is cause by the way threads and processes work in python. Threads use a bunch of safety features to avoid having two threads to try to write to the same variable, and this means that when using threads, the end up waiting for their turn to write to variables for a few milliseconds, adding up to the small difference between the 2 approaches.
Summary:
In this video, we looked at how we can add threading support to a python script to make better use of our processor power. There are still more improvement we can make to our scripts by checking if the thumbnails exists and is up-to-date before doing the conversion, or addind a second progress bar while waiting for tasks to finish to make it clear that our scripts is doing its job.

MODULE 2 WRAP UP: SLOWNESS
Over the last few videos, we have learnt about the many things that can cause slowness, we have not covered absolutely every possible cause, but we did checkout the most common reasons.
We talked about how the first thing to do when faced with a slow system is to identify the bottleneck. To do this, we'll need to understand how each components interact with the system, and what resource is being exhausted.
Sometimes, the root cause is that the hardware is'nt just enough, or maybe there are just too many things happening at the same time.
Other times, the problem might be in the code itself.
When trying to fix a program that's slow, we should avoid code that does expensive operations.
We went over several best practices to help us write better performing code. We discussed when to use the right data structures, how to avoid expensive loops, and how to keep results local by creating a cache for example.
We then spent sometime reviewing complex systems and looked at some examples on how to trouble shoot slowness in this kind of enviroment.
Next time you try to debug a performance problem, we'll be able to think about what the bottleneck is, look for what is exhausting that resource and come up with ideas on how to make things faster.